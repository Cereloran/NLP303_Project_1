{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLP 303 - Natural Language Processing\n",
    "## Task 2\n",
    "### By: Michael Cuffe\n",
    "### Assessment 1\n",
    "### Due: 20/10/2024 23:59"
   ],
   "id": "2b4bfee7ee3232c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install Necessary Packages",
   "id": "6a5dc1d0853c80fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T11:11:06.316330Z",
     "start_time": "2024-10-01T11:10:58.956284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# > NUL 2>&1 is used to suppress the output of the installation as they are very long.\n",
    "# Install the transformers library.\n",
    "!pip install transformers > NUL 2>&1\n",
    "# Install the TensorFlow as it's a dependecy for the transformers library.\n",
    "!pip install tensorflow > NUL 2>&1\n",
    "# Install the TensorFlow Keras API as a fix for an issue with the transformers library.\n",
    "!pip install tf-keras > NUL 2>&1\n",
    "# Install the librosa library for audio processing.\n",
    "!pip install -q transformers librosa > NUL 2>&1\n"
   ],
   "id": "ec2fc259677e4335",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing The Installation of Transformers",
   "id": "2e34753d156c0e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T11:11:19.386356Z",
     "start_time": "2024-10-01T11:11:06.329196Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline",
   "id": "f5cac2a1adb66fc9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Check that transformers is functional.\n",
    "\n",
    "#### Added default models to all pipeline declarations in this case \"google/t5-base\" was added to the list of known models.\n",
    "#### As im running this locally i also had to define the device as 0 to enable GPU usage.\n",
    "#### All pipelines use the recommended default models for the task."
   ],
   "id": "6a53515d0775eb42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T11:11:30.338914Z",
     "start_time": "2024-10-01T11:11:19.836469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator = pipeline(\"translation_en_to_de\", model=\"google-t5/t5-base\", device=0,clean_up_tokenization_spaces=True)\n",
    "print(translator(\"The magic of transformers lies in pre-trained models\"))"
   ],
   "id": "fec397d209670585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Die Magie der Transformatoren liegt in vorgeschulten Modellen'}]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Masked Language Modeling with DistilBERT\n",
    "Initialize the masked language modeling pipeline using distilbert-base-uncased.<br>\n",
    "Provide an example sentence with a masked token.<br>\n",
    "Generate text options to fill the masked input.<br>"
   ],
   "id": "9c98607732b107e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T11:12:57.837278Z",
     "start_time": "2024-10-01T11:11:30.344510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the masked language modeling pipeline\n",
    "mlm_pipeline = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\", device=0)\n",
    "\n",
    "# Example sentence with a masked token\n",
    "sentence = \"The quick brown [MASK] leaps over the [MASK] person on a [MASK].\"\n",
    "\n",
    "# Generate text options to fill the masked input\n",
    "mlm_results = mlm_pipeline(sentence)\n",
    "for result in mlm_results:\n",
    "    print(f\"Option: {result['sequence']}, Score: {result['score']:.4f}\")"
   ],
   "id": "fdc51abc767166f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca05d05903f64010b2cacc2d9382d6ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "540dd95c50ee402fa0470e48547945a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1064e9c66b9450fbe5f6fd1163a0546"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1f63414313e4cc69cf450c481fe3b62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c50b0f2ed26246d69a377f5daecdffd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option: the quick brown fox jumps over the shaggy dog., Score: 0.0777\n",
      "Option: the quick brown fox jumps over the barking dog., Score: 0.0541\n",
      "Option: the quick brown fox jumps over the little dog., Score: 0.0191\n",
      "Option: the quick brown fox jumps over the stray dog., Score: 0.0135\n",
      "Option: the quick brown fox jumps over the startled dog., Score: 0.0131\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with ProsusAI/finbert\n",
    "Locate and download the ProsusAI/finbert model. <br>\n",
    "Select 3 to 5 stock market headlines.<br>\n",
    "Classify the sentiment of the financial content.<br>"
   ],
   "id": "8bdcee4a8c072d89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T11:14:59.447390Z",
     "start_time": "2024-10-01T11:12:57.843224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the sentiment analysis pipeline\n",
    "finbert_pipeline = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device=0)\n",
    "\n",
    "# Stock market headlines\n",
    "headlines = [\n",
    "    \"Cats rally as tech shares rebound\",\n",
    "    \"Cat Predicts Market crash amid economic uncertainty\",\n",
    "    \"Investors optimistic about new feline policies\"\n",
    "    \"Cat elected as mayor in 30 states of America\",\n",
    "    \"Stocks hit all-time low after poor quarterly results\"\n",
    "]\n",
    "\n",
    "# Classify the sentiment of the financial content\n",
    "for headline in headlines:\n",
    "    sentiment = finbert_pipeline(headline)\n",
    "    print(f\"Headline: {headline}, Sentiment: {sentiment[0]['label']}, Score: {sentiment[0]['score']:.4f}\")"
   ],
   "id": "69d397ec33ded5be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a96df5eec83742728ed681435969f56b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53297ddf2a8b4c12ab3c91464d99c8fc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54d1437547014f3bb1a0e1025ca12418"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe7c23390c4747838e169c4e0cd919b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80fcbf0faecf4c9e89498289c38c1d30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Stocks rally as tech shares rebound, Sentiment: negative, Score: 0.6301\n",
      "Headline: Market crashes amid economic uncertainty, Sentiment: negative, Score: 0.8717\n",
      "Headline: Investors optimistic about new fiscal policies, Sentiment: positive, Score: 0.6833\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dialogue Generation with Microsoft DialoGPT-large\n",
    "Download the microsoft/DialoGPT-large model.<br>\n",
    "Use the provided code snippet to initialize the model. <br>\n",
    "Chat for 5 lines or more. <br>"
   ],
   "id": "a9b764ce82972ce8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-01T11:14:59.453361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the DialoGPT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "# Set the Chat range for 5 lines or more\n",
    "chat_history_ids = None\n",
    "for step in range(5):\n",
    "\n",
    "#Ask for User input\n",
    "    user_input = input(\">> User: \")\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "# Append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
    "\n",
    "# Generate a response to the text with it \n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, \n",
    "        max_length=1000, \n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\"DialoGPT: {response}\")"
   ],
   "id": "7357774ad9310c61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2aab1eb707794c6292bf518a95f6c067"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28be7f45e1f24e76a31d5e4558a37861"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1c5a8cc59744080a2ef176e506baddf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24abd516fa854e53a6d749f9e44a5eac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae5499454cc74998bf7faccbaadd8e30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a10f2b565164d6fb289186882083c01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DialoGPT: No, it was The Big Wedding.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Speech Recognition with Facebook Wav2Vec2\n",
    "Review the facebook/wav2vec2-base-960h model. <br>\n",
    "Create a .wav audio file and save it. <br>\n",
    "Use the provided code to transcribe the audio. <br>"
   ],
   "id": "25df393b4f0404d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Load the audio file\n",
    "speech, rate = librosa.load(\"/audio.wav\", sr=16000)\n",
    "\n",
    "# Tokenize the inputs\n",
    "input_values = tokenizer(speech, return_tensors='pt').input_values\n",
    "\n",
    "# Store logits (non-normalized predictions)\n",
    "logits = model(input_values).logits\n",
    "\n",
    "# Store predicted ids\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode the audio to generate text\n",
    "transcriptions = tokenizer.decode(predicted_ids[0])\n",
    "print(transcriptions)"
   ],
   "id": "f251bc1bf76a08a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>\n",
    "End of File"
   ],
   "id": "6a7be9c6c6414b68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
