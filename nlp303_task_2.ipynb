{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLP 303 - Natural Language Processing\n",
    "## Task 2\n",
    "### By: Michael Cuffe\n",
    "### Assessment 1\n",
    "### Due: 20/10/2024 23:59"
   ],
   "id": "2b4bfee7ee3232c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Install Necessary Packages\n",
    "The library installation messages are suppressed by adding > NUL 2>&1 to the end of the pip install command. This is done to prevent the output from being displayed in the notebook for readability. "
   ],
   "id": "6a5dc1d0853c80fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:31:43.347166Z",
     "start_time": "2024-10-20T01:31:28.099079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install transformers > NUL 2>&1\n",
    "!pip install tensorflow > NUL 2>&1\n",
    "!pip install tf-keras > NUL 2>&1\n",
    "!pip install -q transformers librosa > NUL 2>&1"
   ],
   "id": "ec2fc259677e4335",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Testing The Installation of Transformers\n",
    "This code block tests the installation of the transformers library. It is a simple test to ensure that the library is installed correctly and is functioning as intended.\n",
    "\n",
    "This block also doubles as a global declaration of the transformers pipeline. This is done to ensure that the pipeline is available to all code blocks in the notebook."
   ],
   "id": "2e34753d156c0e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:31:27.984144500Z",
     "start_time": "2024-10-19T09:39:05.505171Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline",
   "id": "f5cac2a1adb66fc9",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Checking that transformers is functional.\n",
    "\n",
    "This code block tests the functionality of the transformers library."
   ],
   "id": "6a53515d0775eb42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:31:27.987143200Z",
     "start_time": "2024-10-19T09:39:05.519240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "translator = pipeline(\"translation_en_to_de\", model=\"google-t5/t5-base\", device=0, clean_up_tokenization_spaces=True)\n",
    "print(translator(\"The magic of transformers lies in pre-trained models\"))"
   ],
   "id": "fec397d209670585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Die Magie der Transformatoren liegt in vorgeschulten Modellen'}]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Masked Language Modeling with DistilBERT\n",
    "This code block uses the following steps:<br>\n",
    "1. Initialize the masked language modeling pipeline using distilbert-base-uncased.<br>\n",
    "2. Provide an example sentence with a masked token.<br>\n",
    "3. Generate text options to fill the masked input.<br>\n",
    "4. Print the results in a readable format.<br>"
   ],
   "id": "9c98607732b107e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:31:27.987143200Z",
     "start_time": "2024-10-19T09:39:07.862902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mlm_pipeline = pipeline(\"fill-mask\", model=\"distilbert-base-uncased\", device=0)\n",
    "\n",
    "sentence = \"The quick brown fox leaps over the person on a [MASK].\"\n",
    "\n",
    "mlm_results = mlm_pipeline(sentence)\n",
    "\n",
    "for result in mlm_results:\n",
    "    print(f\"Option: {result['sequence']}, Score: {result['score']:.4f}\")"
   ],
   "id": "fdc51abc767166f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option: the quick brown fox leaps over the person on a leash., Score: 0.1125\n",
      "Option: the quick brown fox leaps over the person on a ledge., Score: 0.0519\n",
      "Option: the quick brown fox leaps over the person on a ladder., Score: 0.0403\n",
      "Option: the quick brown fox leaps over the person on a leap., Score: 0.0364\n",
      "Option: the quick brown fox leaps over the person on a limb., Score: 0.0353\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis with ProsusAI/finbert\n",
    "\n",
    "This code block uses the following steps:<br>\n",
    "1. Initialize the sentiment analysis pipeline.<br>\n",
    "2. Provide a list of stock market headlines.<br>\n",
    "3. Classify the sentiment of the financial content.<br>\n",
    "4. Print the results in a readable format.<br>\n",
    "5. Repeat the process for each headline.<br>"
   ],
   "id": "8bdcee4a8c072d89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:31:27.987143200Z",
     "start_time": "2024-10-19T09:39:08.670071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "finbert_pipeline = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device=0)\n",
    "\n",
    "headlines = [\n",
    "    \"Cats rally as tech shares rebound\",\n",
    "    \"Cat Predicts Market crash amid economic uncertainty\",\n",
    "    \"Investors optimistic about new feline policies\"\n",
    "    \"Cat elected as mayor in 30 states of America\",\n",
    "    \"Stocks hit all-time low after poor quarterly results\"\n",
    "]\n",
    "\n",
    "for headline in headlines:\n",
    "    sentiment = finbert_pipeline(headline)\n",
    "    print(f\"Headline: {headline}, Sentiment: {sentiment[0]['label']}, Score: {sentiment[0]['score']:.4f}\")"
   ],
   "id": "69d397ec33ded5be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Cats rally as tech shares rebound, Sentiment: positive, Score: 0.5534\n",
      "Headline: Cat Predicts Market crash amid economic uncertainty, Sentiment: negative, Score: 0.9009\n",
      "Headline: Investors optimistic about new feline policiesCat elected as mayor in 30 states of America, Sentiment: positive, Score: 0.7586\n",
      "Headline: Stocks hit all-time low after poor quarterly results, Sentiment: negative, Score: 0.9664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dialogue Generation with Microsoft DialoGPT-large\n",
    "\n",
    "Here we use the provided code snippet to initialize the model. <br>\n",
    "\n",
    "The code block uses the following steps:<br>\n",
    "1. Initialize the DialoGPT model and tokenizer.<br>\n",
    "2. Set the Chat range for 5 lines or more.<br>\n",
    "3. Ask for User input.<br>\n",
    "4. Append the new user input tokens to the chat history.<br>\n",
    "5. Generate a response to the text with it.<br>\n",
    "6. Decode the response & print the response.<br>"
   ],
   "id": "a9b764ce82972ce8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "chat_history_ids = None\n",
    "for step in range(5):\n",
    "    user_input = input(\">> User: \")\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
    "\n",
    "    attention_mask = torch.cat([torch.ones_like(chat_history_ids), torch.ones_like(new_user_input_ids)], dim=-1) if chat_history_ids is not None else torch.ones_like(new_user_input_ids)\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, \n",
    "        max_length=1000, \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\"DialoGPT says '{response}'\")"
   ],
   "id": "ec01460db1c20982"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Speech Recognition with Facebook Wav2Vec2\n",
    "This code block review the facebook/wav2vec2-base-960h model. <br>\n",
    "For this i used .wav audio file of Winston Churchills famous speech \"Their Finest Hour\". <br>\n",
    "Sadly my microphone is not working so i could not record my own audio. <br>\n",
    "Use the provided code to transcribe the audio. <br>\n",
    "\n",
    "\n",
    "This code block uses the following steps:<br>\n",
    "1. Load the model and tokenizer.<br>\n",
    "2. Load the audio file.<br>\n",
    "3. Tokenize the inputs.<br>\n",
    "4. Store the logits (non-normalized predictions).<br>\n",
    "5. Store the predicted ids.<br>\n",
    "6. Decode the audio to generate text.<br>\n",
    "7. Print the transcriptions.<br>"
   ],
   "id": "caeb4c5e3c962e0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T01:31:27.988143200Z",
     "start_time": "2024-10-19T11:20:49.934964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\", device=0)\n",
    "\n",
    "speech, rate = librosa.load(\"their_finest_hour.wav\", sr=16000)\n",
    "\n",
    "input_values = tokenizer(speech, return_tensors='pt').input_values\n",
    "\n",
    "logits = model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "transcriptions = tokenizer.decode(predicted_ids[0])\n",
    "print(transcriptions)"
   ],
   "id": "f251bc1bf76a08a0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT CANERAL BAGON AN CALLD THE BATTLE OF BRANZE IS OVER THE BATTLE OF BRITADS IS ABOUT TO BEGIN UPON THIS BATTLE DEPENDS THE SURBIBAL OF CHRISTIAN CIVILIZAG UPON IT DEPENDS OUR OWN BRITISH LIFE AND THE LONG CONTINUITY OF OUR INSTITUTION AND OUR EMPIN O FURY AND MIGHT HAVE THE ENEMY MUST VERY SOON BE TURNED ON US IT ERN OLD THAT HE WILL HAVE TO BREAK US IN THIS ISLAND OR LOVE THE WAR WE CAN STAND UP TO HIM OR EUROPE MAY BE FREE AND THE LIFE OF THE WORLD MAY MOVE FORWARD INTO BROAD UND I UPLAND BUT IF WE FAIL THEN THE WHOLE WORLD INCLUDING THE UNITED STAGE INCLUDING ALL THAT WE HAVE KNOWN AND CAD FORWILL SINK INTO THE ABYSS OF A NEW DARK AGE MADE MORE SINISTER AN THE HEAT MORE PROTRECTIVE BY THE LIGHT OF THE VIRTI SIEND LET US THEREFORE BREAK OURSELVES TO OUR DUTY SO BEAR OURSELVES THAT IF THE BRITISH EMPIRE AND ITS COMMONWEALTH LAST FOR AFOUND THEER MEN WILL STILL SAY LESH WAT THEY ARE FINDT OUR\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "<br>\n",
    "End of File"
   ],
   "id": "6a7be9c6c6414b68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
